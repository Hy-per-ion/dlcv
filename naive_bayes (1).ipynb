{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PW3hdpj1Yvdx",
    "outputId": "28aeb35b-1a30-42c1-d989-bacc9691f08f",
    "ExecuteTime": {
     "end_time": "2024-05-08T18:59:39.434565Z",
     "start_time": "2024-05-08T18:59:39.376416Z"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "\n",
    "# Download the necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Removing stopwords and non-alphabetic tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]  # Stemming\n",
    "    return dict([(token, True) for token in tokens])\n",
    "\n",
    "# Read dataset from a text file\n",
    "dataset_file = \"dataset.txt\"  # Path to your dataset file\n",
    "dataset = []\n",
    "with open(dataset_file, 'r') as file:\n",
    "    for line in file:\n",
    "        text, label = line.strip().split(\",\")\n",
    "        dataset.append((text, label))\n",
    "\n",
    "# Preprocess the dataset\n",
    "preprocessed_dataset = [(preprocess(text), label) for text, label in dataset]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data = preprocessed_dataset[:90]\n",
    "test_data = preprocessed_dataset[10:]\n",
    "\n",
    "# Train the Naive Bayes Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# Test the classifier\n",
    "print(\"Accuracy:\", accuracy(classifier, test_data))\n",
    "\n",
    "# Test a new text segment\n",
    "text_to_classify = \"The product exceeded my expectations\"  # You can change this text to test different segments\n",
    "preprocessed_text = preprocess(text_to_classify)\n",
    "print(\"Classification:\", classifier.classify(preprocessed_text))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n",
      "Classification: positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "oXbhCzqZY5ed",
    "ExecuteTime": {
     "end_time": "2024-05-08T18:59:39.438624Z",
     "start_time": "2024-05-08T18:59:39.434565Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  }
 ]
}
